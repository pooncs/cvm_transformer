#!/usr/bin/env python3
"""
Test the advanced trained model with learning rate scheduling
"""

import torch
import sentencepiece as spm
import json
from typing import List, Dict
import time

def load_advanced_model(model_path: str, device: str):
    """Load the advanced trained model"""
    
    # Import the model architecture
    from train_enhanced import EnhancedTransformer
    
    # Model configuration (must match training)
    config = {
        'vocab_size': 1000,
        'd_model': 512,
        'nhead': 8,
        'num_encoder_layers': 12,
        'num_decoder_layers': 12,
        'dim_feedforward': 2048,
        'dropout': 0.1
    }
    
    model = EnhancedTransformer(**config).to(device)
    
    # Load checkpoint
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model.eval()
    return model

def translate_advanced(model, tokenizer, korean_text: str, device: str, max_length: int = 128):
    """Translate Korean to English using the advanced model"""
    
    # Tokenize input
    src_tokens = tokenizer.encode(korean_text, out_type=int)
    src_tokens = [1] + src_tokens + [2]  # Add SOS and EOS
    
    # Pad to max_length
    while len(src_tokens) < max_length:
        src_tokens.append(0)
    src_tokens = src_tokens[:max_length]
    
    src_tensor = torch.tensor([src_tokens]).to(device)
    
    # Generate translation
    with torch.no_grad():
        # Initialize target with SOS token
        tgt_tokens = [2]  # SOS token
        
        for _ in range(max_length - 1):
            # Prepare target tensor
            tgt_input = tgt_tokens + [0] * (max_length - len(tgt_tokens))
            tgt_input = tgt_input[:max_length]
            tgt_tensor = torch.tensor([tgt_input]).to(device)
            
            # Get model output
            output = model(src_tensor, tgt_tensor)
            
            # Get next token
            next_token_logits = output[0, len(tgt_tokens) - 1]
            next_token = next_token_logits.argmax().item()
            
            # Check for EOS token
            if next_token == 3:  # EOS token
                break
                
            tgt_tokens.append(next_token)
    
    # Decode translation
    translation_tokens = tgt_tokens[1:]  # Remove SOS
    translation = tokenizer.decode(translation_tokens)
    
    return translation

def calculate_word_overlap_score(predicted: str, expected: str) -> float:
    """Calculate word overlap score between predicted and expected translations"""
    
    pred_words = set(predicted.lower().split())
    expected_words = set(expected.lower().split())
    
    if not expected_words:
        return 1.0 if not pred_words else 0.0
    
    overlap = len(pred_words & expected_words)
    return overlap / len(expected_words)

def main():
    """Main test function"""
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")
    
    # Load tokenizer
    tokenizer = spm.SentencePieceProcessor(model_file='kr_en_diverse.model')
    print(f"Tokenizer vocab size: {tokenizer.vocab_size()}")
    
    # Load model
    print("Loading advanced model...")
    model = load_advanced_model('best_advanced_simple_model.pth', device)
    print("‚úÖ Advanced model loaded successfully")
    
    # Test sentences
    test_sentences = [
        ("ÏïàÎÖïÌïòÏÑ∏Ïöî", "Hello"),
        ("Í∞êÏÇ¨Ìï©ÎãàÎã§", "Thank you"),
        ("ÏïàÎÖïÌûà Í∞ÄÏÑ∏Ïöî", "Goodbye"),
        ("Ïò§Îäò ÎÇ†Ïî®Í∞Ä Ï†ïÎßê Ï¢ãÎÑ§Ïöî", "The weather is really nice today"),
        ("Ï†ÄÎäî Ïª§ÌîºÎ•º Ï¢ãÏïÑÌï©ÎãàÎã§", "I like coffee"),
        ("ÏïÑÏπ®Ïóê ÏùºÏ∞ç ÏùºÏñ¥ÎÇ¨Ïñ¥Ïöî", "I woke up early in the morning"),
        ("ÌöåÏùòÍ∞Ä Î™á ÏãúÏóê ÏûàÎÇòÏöî?", "What time is the meeting?"),
        ("ÏñºÎßàÏòàÏöî?", "How much is it?"),
        ("ÌôîÏû•Ïã§Ïù¥ Ïñ¥ÎîîÏóê ÏûàÎÇòÏöî?", "Where is the bathroom?"),
        ("ÌïúÍµ≠Ïóê Ïò® ÏßÄ 3Í∞úÏõîÏù¥ ÎêòÏóàÍ≥† Ï†êÏ†ê ÌïúÍµ≠Ïñ¥Î•º ÏûòÌïòÍ≤å ÎêòÍ≥† ÏûàÏñ¥Ïöî", "It's been 3 months since I came to Korea and I'm gradually getting better at Korean"),
        ("ÎπÑÍ∞Ä Ïò§Í≥† ÏûàÏñ¥ÏÑú Ïö∞ÏÇ∞ÏùÑ Í∞ÄÏ†∏Í∞ÄÎäî Í≤å Ï¢ãÏùÑ Í≤É Í∞ôÏïÑÏöî", "Since it's raining, it would be good to take an umbrella"),
        ("ÎßõÏûàÎäî ÏãùÎãπÏùÑ Ï∂îÏ≤úÌï¥ Ï£ºÏÑ∏Ïöî", "Please recommend a delicious restaurant"),
        ("Ïù¥ ÏùåÏãùÏù¥ ÎÑàÎ¨¥ ÏßúÏöî", "This food is too salty"),
        ("Í≥ÑÏÇ∞ÏÑú Ï£ºÏÑ∏Ïöî", "Please give me the bill"),
        ("Ïò§Îäò Í∏∞Î∂ÑÏù¥ Ï†ïÎßê Ï¢ãÏïÑÏöî", "I feel really good today"),
        ("Í±±Ï†ïÏù¥ ÎßéÏù¥ ÎêòÎÑ§Ïöî", "I'm very worried"),
        ("Ïù¥ ÏÜåÏãùÏù¥ ÎÑàÎ¨¥ Í∏∞ÏÅ©ÎãàÎã§", "This news makes me very happy")
    ]
    
    print(f"\nüß™ Testing {len(test_sentences)} sentence pairs")
    print("=" * 60)
    
    results = []
    total_score = 0
    
    for i, (korean, expected) in enumerate(test_sentences, 1):
        print(f"\nTest {i}:")
        print(f"üá∞üá∑ Korean: {korean}")
        print(f"üá∫üá∏ Expected: {expected}")
        
        # Translate
        start_time = time.time()
        translation = translate_advanced(model, tokenizer, korean, device)
        translation_time = time.time() - start_time
        
        # Calculate score
        score = calculate_word_overlap_score(translation, expected)
        total_score += score
        
        print(f"ü§ñ Translation: {translation}")
        print(f"üìä Word overlap: {score:.2f}")
        print(f"‚è±Ô∏è Translation time: {translation_time:.3f}s")
        
        results.append({
            'korean': korean,
            'expected': expected,
            'translation': translation,
            'score': score,
            'time': translation_time
        })
    
    # Summary
    avg_score = total_score / len(test_sentences)
    print(f"\nüìà OVERALL RESULTS")
    print("=" * 40)
    print(f"Average translation score: {avg_score:.3f}")
    print(f"Tests passed (>0.3 score): {sum(1 for r in results if r['score'] > 0.3)}/{len(results)}")
    print(f"Tests with good overlap (>0.5): {sum(1 for r in results if r['score'] > 0.5)}/{len(results)}")
    
    # Compare with previous results
    try:
        with open('comprehensive_test_results.json', 'r') as f:
            previous_results = json.load(f)
            previous_score = previous_results.get('average_score', 0)
            print(f"\nüîÑ COMPARISON WITH PREVIOUS MODEL:")
            print(f"Previous average score: {previous_score:.3f}")
            print(f"Advanced model score: {avg_score:.3f}")
            print(f"Improvement: {avg_score - previous_score:.3f}")
    except:
        print("\n‚ÑπÔ∏è No previous results to compare with")
    
    # Save results
    with open('advanced_model_test_results.json', 'w') as f:
        json.dump({
            'average_score': avg_score,
            'results': results,
            'model': 'best_advanced_simple_model.pth'
        }, f, indent=2)
    
    print(f"\nüíæ Results saved to advanced_model_test_results.json")
    
    return avg_score

if __name__ == "__main__":
    main()