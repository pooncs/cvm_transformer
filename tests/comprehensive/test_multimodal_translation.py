import torch
import torch.nn as nn
import numpy as np
from typing import List, Dict, Tuple, Optional, Any
import json
import time
import os
import sys
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import soundfile as sf
from PIL import Image, ImageDraw, ImageFont
import io
import base64
from datetime import datetime

# Add the project root to Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Import our models
from src.models.nmt_transformer import create_nmt_transformer as create_nmt_model
from src.models.multimodal_encoders import create_multimodal_model
from src.data.prepare_corpus import ParallelCorpusProcessor
from src.training.train_nmt import NMTTrainer
from src.utils.metrics import BLEUScore, ExactMatchScore, SemanticSimilarity


class MultimodalTestSuite:
    """Comprehensive test suite for Korean-English translation with multimodal inputs."""
    
    def __init__(
        self,
        model_path: str,
        tokenizer_path: str,
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    ):
        self.device = device
        self.model_path = model_path
        self.tokenizer_path = tokenizer_path
        
        # Initialize models
        self.text_model = None
        self.multimodal_model = None
        self.tokenizer = None
        
        # Metrics
        self.bleu_scorer = BLEUScore()
        self.exact_match_scorer = ExactMatchScore()
        self.semantic_scorer = SemanticSimilarity()
        
        # Test results
        self.results = {
            'text_tests': [],
            'image_tests': [],
            'audio_tests': [],
            'multimodal_tests': [],
            'summary': {}
        }
        
        # Load models and tokenizer
        self._load_models()
        
    def _load_models(self):
        """Load pre-trained models and tokenizer."""
        try:
            # Load tokenizer
            self.tokenizer = torch.load(self.tokenizer_path, map_location=self.device)
            
            # Load text model
            checkpoint = torch.load(f"{self.model_path}/text_model_best.pt", map_location=self.device)
            vocab_size = checkpoint['vocab_size']
            
            # Create text model config
            text_config = {
                'src_vocab_size': vocab_size,
                'tgt_vocab_size': vocab_size,
                'd_model': 1024,
                'n_heads': 16,
                'n_encoder_layers': 12,
                'n_decoder_layers': 12,
                'd_ff': 4096,
                'max_len': 512,
                'dropout': 0.1,
                'pad_id': 0,
                'use_flash_attention': True
            }
            
            self.text_model = create_nmt_model(text_config)
            self.text_model.load_state_dict(checkpoint['model_state_dict'])
            self.text_model.to(self.device)
            self.text_model.eval()
            
            # Load multimodal model
            multimodal_checkpoint = torch.load(f"{self.model_path}/multimodal_model_best.pt", map_location=self.device)
            self.multimodal_model = create_multimodal_model(
                src_vocab_size=vocab_size,
                tgt_vocab_size=vocab_size,
                d_model=1024,
                n_heads=16,
                n_encoder_layers=12,
                n_decoder_layers=12,
                d_ff=4096,
                max_len=512,
                dropout=0.1,
                pad_id=0,
                use_flash=True
            )
            self.multimodal_model.load_state_dict(multimodal_checkpoint['model_state_dict'])
            self.multimodal_model.to(self.device)
            self.multimodal_model.eval()
            
            print(f"‚úÖ Models loaded successfully on {self.device}")
            
        except Exception as e:
            print(f"‚ùå Error loading models: {e}")
            print("Creating new models for testing...")
            self._create_new_models()
    
    def _create_new_models(self):
        """Create new models for testing when pre-trained models are not available."""
        vocab_size = 32000  # Default vocab size
        
        # Create text model config
        text_config = {
            'src_vocab_size': vocab_size,
            'tgt_vocab_size': vocab_size,
            'd_model': 1024,
            'n_heads': 16,
            'n_encoder_layers': 12,
            'n_decoder_layers': 12,
            'd_ff': 4096,
            'max_len': 512,
            'dropout': 0.1,
            'pad_id': 0,
            'use_flash_attention': True
        }
        
        # Create text model
        self.text_model = create_nmt_model(text_config)
        self.text_model.to(self.device)
        
        # Create multimodal model
        self.multimodal_model = create_multimodal_model(
            src_vocab_size=vocab_size,
            tgt_vocab_size=vocab_size,
            d_model=1024,
            n_heads=16,
            n_encoder_layers=12,
            n_decoder_layers=12,
            d_ff=4096,
            max_len=512,
            dropout=0.1,
            pad_id=0,
            use_flash=True
        )
        self.multimodal_model.to(self.device)
        
        print("üÜï New models created for testing")
    
    def _tokenize(self, text: str) -> torch.Tensor:
        """Tokenize text using the loaded tokenizer."""
        if self.tokenizer is None:
            # Fallback tokenization
            tokens = text.split()
            token_ids = [hash(token) % 30000 + 1 for token in tokens]  # Simple hash-based tokenization
            return torch.tensor([token_ids], device=self.device)
        
        return self.tokenizer.encode(text, return_tensors='pt').to(self.device)
    
    def _detokenize(self, token_ids: torch.Tensor) -> str:
        """Detokenize token IDs back to text."""
        if self.tokenizer is None:
            # Fallback detokenization
            tokens = [f"token_{idx.item()}" for idx in token_ids[0]]
            return " ".join(tokens)
        
        return self.tokenizer.decode(token_ids[0], skip_special_tokens=True)
    
    def generate_test_images(self) -> List[Tuple[np.ndarray, str, str]]:
        """Generate synthetic test images with Korean text."""
        test_cases = [
            ("ÏïàÎÖïÌïòÏÑ∏Ïöî", "Hello", "Basic greeting"),
            ("Í∞êÏÇ¨Ìï©ÎãàÎã§", "Thank you", "Expression of gratitude"),
            ("ÏÇ¨ÎûëÌï¥Ïöî", "I love you", "Expression of love"),
            ("Ïò§Îäò ÎÇ†Ïî®Í∞Ä Ï¢ãÎÑ§Ïöî", "The weather is nice today", "Weather description"),
            ("ÌïôÍµêÏóê Í∞ÄÏöî", "I'm going to school", "Daily activity"),
            ("Î∞• Î®πÏóàÏñ¥Ïöî?", "Did you eat?", "Common question"),
            ("Ï¢ãÏùÄ ÏïÑÏπ®ÏûÖÎãàÎã§", "Good morning", "Morning greeting"),
            ("ÏïàÎÖïÌûà Í∞ÄÏÑ∏Ïöî", "Goodbye", "Farewell"),
            ("Ï£ÑÏÜ°Ìï©ÎãàÎã§", "I'm sorry", "Apology"),
            ("Ï∂ïÌïòÌï©ÎãàÎã§", "Congratulations", "Congratulations")
        ]
        
        images_with_text = []
        
        for korean_text, english_translation, description in test_cases:
            # Create image with Korean text
            img = Image.new('RGB', (224, 224), color='white')
            draw = ImageDraw.Draw(img)
            
            # Try to use a Korean font, fallback to default
            try:
                font = ImageFont.truetype("malgun.ttf", 24)
            except:
                try:
                    font = ImageFont.load_default()
                except:
                    font = None
            
            # Draw text
            if font:
                # Center the text
                bbox = draw.textbbox((0, 0), korean_text, font=font)
                text_width = bbox[2] - bbox[0]
                text_height = bbox[3] - bbox[1]
                x = (224 - text_width) // 2
                y = (224 - text_height) // 2
                draw.text((x, y), korean_text, font=font, fill='black')
            else:
                # Fallback: draw rectangles to simulate text
                for i, char in enumerate(korean_text):
                    x = 20 + (i % 8) * 25
                    y = 50 + (i // 8) * 30
                    draw.rectangle([x, y, x+20, y+20], fill='black')
            
            # Add some noise and variations
            img_array = np.array(img)
            
            # Add slight rotation
            if np.random.random() > 0.5:
                img_array = np.rot90(img_array, k=np.random.randint(0, 4))
            
            # Add brightness variation
            brightness_factor = np.random.uniform(0.8, 1.2)
            img_array = np.clip(img_array * brightness_factor, 0, 255).astype(np.uint8)
            
            images_with_text.append((img_array, english_translation, description))
        
        return images_with_text
    
    def generate_test_audio(self) -> List[Tuple[np.ndarray, str, str]]:
        """Generate synthetic test audio with Korean speech patterns."""
        test_cases = [
            ("ÏïàÎÖïÌïòÏÑ∏Ïöî", "Hello", "Basic greeting"),
            ("Í∞êÏÇ¨Ìï©ÎãàÎã§", "Thank you", "Expression of gratitude"),
            ("ÏÇ¨ÎûëÌï¥Ïöî", "I love you", "Expression of love"),
            ("Ïò§Îäò ÎÇ†Ïî®Í∞Ä Ï¢ãÎÑ§Ïöî", "The weather is nice today", "Weather description"),
            ("ÌïôÍµêÏóê Í∞ÄÏöî", "I'm going to school", "Daily activity")
        ]
        
        audio_samples = []
        
        for korean_text, english_translation, description in test_cases:
            # Generate synthetic speech-like signal
            duration = len(korean_text) * 0.2  # Rough duration estimate
            sample_rate = 16000
            t = np.linspace(0, duration, int(sample_rate * duration))
            
            # Create multiple harmonics to simulate speech
            fundamental = 100 + np.random.randint(-20, 20)  # Base frequency
            signal = np.zeros_like(t)
            
            # Add harmonics
            for harmonic in range(1, 6):
                freq = fundamental * harmonic
                amplitude = 1.0 / harmonic
                phase = np.random.uniform(0, 2 * np.pi)
                signal += amplitude * np.sin(2 * np.pi * freq * t + phase)
            
            # Add formant-like frequency modulation
            formant_freq = 2000 + np.random.randint(-500, 500)
            formant = 0.3 * np.sin(2 * np.pi * formant_freq * t)
            signal += formant
            
            # Add envelope to simulate syllables
            envelope = np.ones_like(t)
            syllable_duration = 0.2
            n_syllables = int(duration / syllable_duration)
            for i in range(n_syllables):
                start_idx = int(i * syllable_duration * sample_rate)
                end_idx = min(int((i + 1) * syllable_duration * sample_rate), len(t))
                if end_idx > start_idx:
                    syllable_env = np.linspace(0, 1, end_idx - start_idx)
                    syllable_env *= np.linspace(1, 0.3, end_idx - start_idx)
                    envelope[start_idx:end_idx] = syllable_env
            
            signal *= envelope
            
            # Add noise
            noise = np.random.normal(0, 0.05, signal.shape)
            signal += noise
            
            # Normalize
            signal = signal / np.max(np.abs(signal))
            
            audio_samples.append((signal, english_translation, description))
        
        return audio_samples
    
    def run_text_tests(self) -> Dict[str, Any]:
        """Run comprehensive text-based translation tests."""
        print("üìù Running text translation tests...")
        
        test_cases = [
            # Basic phrases
            ("ÏïàÎÖïÌïòÏÑ∏Ïöî", "Hello"),
            ("Í∞êÏÇ¨Ìï©ÎãàÎã§", "Thank you"),
            ("ÏÇ¨ÎûëÌï¥Ïöî", "I love you"),
            ("Ï£ÑÏÜ°Ìï©ÎãàÎã§", "I'm sorry"),
            ("Ï∂ïÌïòÌï©ÎãàÎã§", "Congratulations"),
            
            # Complex sentences
            ("Ïò§Îäò ÎÇ†Ïî®Í∞Ä Ï†ïÎßê Ï¢ãÎÑ§Ïöî.", "The weather is really nice today."),
            ("ÌïôÍµêÏóê Í∞ÄÏÑú ÏπúÍµ¨Îì§ÏùÑ ÎßåÎÇ¨Ïñ¥Ïöî.", "I went to school and met my friends."),
            ("Î∞•ÏùÑ Î®πÍ≥† ÎÇòÏÑú Í≥µÎ∂ÄÎ•º ÌñàÏñ¥Ïöî.", "After eating, I studied."),
            ("Ï£ºÎßêÏóê ÏòÅÌôîÎ•º Î¥§Ïñ¥Ïöî.", "I watched a movie on the weekend."),
            ("ÌïúÍµ≠Ïñ¥Î•º Î∞∞Ïö∞Í≥† Ïã∂Ïñ¥Ïöî.", "I want to learn Korean."),
            
            # Domain-specific
            ("Ïù¥ ÏïΩÏùÄ ÌïòÎ£®Ïóê ÏÑ∏ Î≤à Î≥µÏö©ÌïòÏÑ∏Ïöî.", "Take this medicine three times a day."),
            ("ÌöåÏùòÎäî Ïò§ÌõÑ 3ÏãúÏóê ÏãúÏûëÌï©ÎãàÎã§.", "The meeting starts at 3 PM."),
            ("Ïù¥ Ï†úÌíàÏùÄ 100% Ï≤úÏó∞ Ïû¨Î£åÎ°ú ÎßåÎì§Ïñ¥Ï°åÏäµÎãàÎã§.", "This product is made with 100% natural ingredients."),
            ("ÎπÑÌñâÍ∏∞Í∞Ä 30Î∂Ñ ÌõÑÏóê Ï∂úÎ∞úÌï©ÎãàÎã§.", "The plane departs in 30 minutes."),
            ("Ïù¥ Ï±ÖÏùÄ ÌïúÍµ≠ Ïó≠ÏÇ¨Ïóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï©ÎãàÎã§.", "This book explains Korean history."),
            
            # Colloquial expressions
            ("Ïñ¥ÎñªÍ≤å ÏßÄÎÇ¥ÏÑ∏Ïöî?", "How are you?"),
            ("Î≥ÑÏùº ÏóÜÏßÄÏöî?", "Nothing special?"),
            ("ÎßõÏûàÍ≤å ÎìúÏÑ∏Ïöî.", "Enjoy your meal."),
            ("Îì§Ïñ¥Ïò§ÏÑ∏Ïöî.", "Please come in."),
            ("Ï°∞Ïã¨Ìï¥ÏÑú Í∞ÄÏÑ∏Ïöî.", "Go carefully.")
        ]
        
        text_results = []
        
        for korean_text, expected_translation in test_cases:
            start_time = time.time()
            
            try:
                # Tokenize input
                src_tokens = self._tokenize(korean_text)
                
                # Generate translation
                with torch.no_grad():
                    # Get encoder output first
                    src_mask = self.text_model.create_padding_mask(src_tokens)
                    encoder_out = self.text_model.encoder(src_tokens, src_mask)
                    
                    # Generate using encoder output
                    generated = self.text_model.generate(
                        encoder_out,
                        src_mask,
                        batch_size=src_tokens.size(0),
                        device=src_tokens.device,
                        max_len=len(expected_translation.split()) + 10,
                        beam_size=4,
                        temperature=0.8
                    )
                
                # Decode output
                translation = self._detokenize(generated)
                
                # Calculate metrics
                bleu_score = self.bleu_scorer([expected_translation], [translation])
                exact_match = self.exact_match_scorer(expected_translation, translation)
                semantic_sim = self.semantic_scorer(expected_translation, translation)
                
                execution_time = time.time() - start_time
                
                result = {
                    'input': korean_text,
                    'expected': expected_translation,
                    'predicted': translation,
                    'bleu_score': bleu_score,
                    'exact_match': exact_match,
                    'semantic_similarity': semantic_sim,
                    'execution_time': execution_time,
                    'perfect_translation': bleu_score > 0.8 and exact_match == 1.0,
                    'test_type': 'text'
                }
                
                text_results.append(result)
                
                print(f"‚úÖ {korean_text} ‚Üí {translation} (BLEU: {bleu_score:.3f})")
                
            except Exception as e:
                print(f"‚ùå Error testing '{korean_text}': {e}")
                result = {
                    'input': korean_text,
                    'expected': expected_translation,
                    'predicted': "ERROR",
                    'bleu_score': 0.0,
                    'exact_match': 0,
                    'semantic_similarity': 0.0,
                    'execution_time': time.time() - start_time,
                    'perfect_translation': False,
                    'test_type': 'text',
                    'error': str(e)
                }
                text_results.append(result)
        
        return {
            'test_type': 'text',
            'total_tests': len(text_results),
            'results': text_results,
            'average_bleu': np.mean([r['bleu_score'] for r in text_results]),
            'perfect_rate': np.mean([r['perfect_translation'] for r in text_results]),
            'average_time': np.mean([r['execution_time'] for r in text_results])
        }
    
    def run_image_tests(self) -> Dict[str, Any]:
        """Run image-based translation tests."""
        print("üñºÔ∏è Running image translation tests...")
        
        # Generate test images
        test_images = self.generate_test_images()
        image_results = []
        
        for image_array, expected_translation, description in test_images:
            start_time = time.time()
            
            try:
                # Convert image to tensor
                image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).float() / 255.0
                image_tensor = image_tensor.unsqueeze(0).to(self.device)
                
                # Create dummy text input (since we need both text and image)
                dummy_text = "image"
                src_tokens = self._tokenize(dummy_text)
                
                # Generate translation using multimodal model
                with torch.no_grad():
                    generated = self.multimodal_model.generate(
                        src_tokens,
                        src_images=image_tensor,
                        max_len=len(expected_translation.split()) + 10,
                        beam_size=4,
                        temperature=0.8
                    )
                
                # Decode output
                translation = self._detokenize(generated)
                
                # Calculate metrics
                bleu_score = self.bleu_scorer([expected_translation], [translation])
                exact_match = self.exact_match_scorer(expected_translation, translation)
                semantic_sim = self.semantic_scorer(expected_translation, translation)
                
                execution_time = time.time() - start_time
                
                result = {
                    'input_image_shape': image_array.shape,
                    'expected': expected_translation,
                    'predicted': translation,
                    'description': description,
                    'bleu_score': bleu_score,
                    'exact_match': exact_match,
                    'semantic_similarity': semantic_sim,
                    'execution_time': execution_time,
                    'perfect_translation': bleu_score > 0.8 and exact_match == 1.0,
                    'test_type': 'image'
                }
                
                image_results.append(result)
                print(f"üñºÔ∏è Image test: {description} ‚Üí {translation} (BLEU: {bleu_score:.3f})")
                
            except Exception as e:
                print(f"‚ùå Error in image test '{description}': {e}")
                result = {
                    'input_image_shape': image_array.shape,
                    'expected': expected_translation,
                    'predicted': "ERROR",
                    'description': description,
                    'bleu_score': 0.0,
                    'exact_match': 0,
                    'semantic_similarity': 0.0,
                    'execution_time': time.time() - start_time,
                    'perfect_translation': False,
                    'test_type': 'image',
                    'error': str(e)
                }
                image_results.append(result)
        
        return {
            'test_type': 'image',
            'total_tests': len(image_results),
            'results': image_results,
            'average_bleu': np.mean([r['bleu_score'] for r in image_results]),
            'perfect_rate': np.mean([r['perfect_translation'] for r in image_results]),
            'average_time': np.mean([r['execution_time'] for r in image_results])
        }
    
    def run_audio_tests(self) -> Dict[str, Any]:
        """Run audio-based translation tests."""
        print("üéµ Running audio translation tests...")
        
        # Generate test audio
        test_audio = self.generate_test_audio()
        audio_results = []
        
        for audio_signal, expected_translation, description in test_audio:
            start_time = time.time()
            
            try:
                # Convert audio to tensor
                audio_tensor = torch.from_numpy(audio_signal).float().unsqueeze(0).unsqueeze(0)
                audio_tensor = audio_tensor.to(self.device)
                
                # Create dummy text input
                dummy_text = "audio"
                src_tokens = self._tokenize(dummy_text)
                
                # Generate translation using multimodal model
                with torch.no_grad():
                    generated = self.multimodal_model.generate(
                        src_tokens,
                        src_audio=audio_tensor,
                        max_len=len(expected_translation.split()) + 10,
                        beam_size=4,
                        temperature=0.8
                    )
                
                # Decode output
                translation = self._detokenize(generated)
                
                # Calculate metrics
                bleu_score = self.bleu_scorer([expected_translation], [translation])
                exact_match = self.exact_match_scorer(expected_translation, translation)
                semantic_sim = self.semantic_scorer(expected_translation, translation)
                
                execution_time = time.time() - start_time
                
                result = {
                    'input_audio_shape': audio_signal.shape,
                    'expected': expected_translation,
                    'predicted': translation,
                    'description': description,
                    'bleu_score': bleu_score,
                    'exact_match': exact_match,
                    'semantic_similarity': semantic_sim,
                    'execution_time': execution_time,
                    'perfect_translation': bleu_score > 0.8 and exact_match == 1.0,
                    'test_type': 'audio'
                }
                
                audio_results.append(result)
                print(f"üéµ Audio test: {description} ‚Üí {translation} (BLEU: {bleu_score:.3f})")
                
            except Exception as e:
                print(f"‚ùå Error in audio test '{description}': {e}")
                result = {
                    'input_audio_shape': audio_signal.shape,
                    'expected': expected_translation,
                    'predicted': "ERROR",
                    'description': description,
                    'bleu_score': 0.0,
                    'exact_match': 0,
                    'semantic_similarity': 0.0,
                    'execution_time': time.time() - start_time,
                    'perfect_translation': False,
                    'test_type': 'audio',
                    'error': str(e)
                }
                audio_results.append(result)
        
        return {
            'test_type': 'audio',
            'total_tests': len(audio_results),
            'results': audio_results,
            'average_bleu': np.mean([r['bleu_score'] for r in audio_results]),
            'perfect_rate': np.mean([r['perfect_translation'] for r in audio_results]),
            'average_time': np.mean([r['execution_time'] for r in audio_results])
        }
    
    def run_multimodal_tests(self) -> Dict[str, Any]:
        """Run combined multimodal tests (text + image + audio)."""
        print("üîä Running multimodal translation tests...")
        
        # Create multimodal test cases
        test_cases = [
            {
                'text': "ÏïàÎÖïÌïòÏÑ∏Ïöî",
                'image_expected': "Hello",
                'audio_expected': "Hello",
                'combined_expected': "Hello",
                'description': "Basic greeting across all modalities"
            },
            {
                'text': "Í∞êÏÇ¨Ìï©ÎãàÎã§",
                'image_expected': "Thank you",
                'audio_expected': "Thank you", 
                'combined_expected': "Thank you",
                'description': "Gratitude expression across all modalities"
            }
        ]
        
        multimodal_results = []
        
        for test_case in test_cases:
            start_time = time.time()
            
            try:
                # Generate image and audio
                images = self.generate_test_images()
                audio_samples = self.generate_test_audio()
                
                # Use first image and audio
                image_tensor = torch.from_numpy(images[0][0]).permute(2, 0, 1).float() / 255.0
                image_tensor = image_tensor.unsqueeze(0).to(self.device)
                
                audio_tensor = torch.from_numpy(audio_samples[0][0]).float().unsqueeze(0).unsqueeze(0)
                audio_tensor = audio_tensor.to(self.device)
                
                # Tokenize text
                src_tokens = self._tokenize(test_case['text'])
                
                # Generate translation using all modalities
                with torch.no_grad():
                    generated = self.multimodal_model.generate(
                        src_tokens,
                        src_images=image_tensor,
                        src_audio=audio_tensor,
                        max_len=len(test_case['combined_expected'].split()) + 10,
                        beam_size=4,
                        temperature=0.8
                    )
                
                # Decode output
                translation = self._detokenize(generated)
                
                # Calculate metrics
                bleu_score = self.bleu_scorer([test_case['combined_expected']], [translation])
                exact_match = self.exact_match_scorer(test_case['combined_expected'], translation)
                semantic_sim = self.semantic_scorer(test_case['combined_expected'], translation)
                
                execution_time = time.time() - start_time
                
                result = {
                    'input_text': test_case['text'],
                    'expected': test_case['combined_expected'],
                    'predicted': translation,
                    'description': test_case['description'],
                    'bleu_score': bleu_score,
                    'exact_match': exact_match,
                    'semantic_similarity': semantic_sim,
                    'execution_time': execution_time,
                    'perfect_translation': bleu_score > 0.8 and exact_match == 1.0,
                    'test_type': 'multimodal'
                }
                
                multimodal_results.append(result)
                print(f"üîä Multimodal test: {test_case['description']} ‚Üí {translation} (BLEU: {bleu_score:.3f})")
                
            except Exception as e:
                print(f"‚ùå Error in multimodal test '{test_case['description']}': {e}")
                result = {
                    'input_text': test_case['text'],
                    'expected': test_case['combined_expected'],
                    'predicted': "ERROR",
                    'description': test_case['description'],
                    'bleu_score': 0.0,
                    'exact_match': 0,
                    'semantic_similarity': 0.0,
                    'execution_time': time.time() - start_time,
                    'perfect_translation': False,
                    'test_type': 'multimodal',
                    'error': str(e)
                }
                multimodal_results.append(result)
        
        return {
            'test_type': 'multimodal',
            'total_tests': len(multimodal_results),
            'results': multimodal_results,
            'average_bleu': np.mean([r['bleu_score'] for r in multimodal_results]),
            'perfect_rate': np.mean([r['perfect_translation'] for r in multimodal_results]),
            'average_time': np.mean([r['execution_time'] for r in multimodal_results])
        }
    
    def run_comprehensive_tests(self) -> Dict[str, Any]:
        """Run all test types and generate comprehensive report."""
        print("üöÄ Starting comprehensive multimodal translation tests...")
        start_time = time.time()
        
        # Run all test types
        text_results = self.run_text_tests()
        image_results = self.run_image_tests()
        audio_results = self.run_audio_tests()
        multimodal_results = self.run_multimodal_tests()
        
        # Compile results
        all_results = {
            'text': text_results,
            'image': image_results,
            'audio': audio_results,
            'multimodal': multimodal_results
        }
        
        # Calculate overall statistics
        total_tests = sum(results['total_tests'] for results in all_results.values())
        overall_bleu = np.mean([results['average_bleu'] for results in all_results.values()])
        overall_perfect_rate = np.mean([results['perfect_rate'] for results in all_results.values()])
        total_time = time.time() - start_time
        
        summary = {
            'total_tests': total_tests,
            'overall_average_bleu': overall_bleu,
            'overall_perfect_translation_rate': overall_perfect_rate,
            'total_execution_time': total_time,
            'tests_per_second': total_tests / total_time,
            'target_achieved': overall_perfect_rate >= 0.99,
            'improvement_needed': max(0.0, 0.99 - overall_perfect_rate)
        }
        
        # Store results
        self.results = {
            'test_results': all_results,
            'summary': summary,
            'timestamp': datetime.now().isoformat()
        }
        
        # Print summary
        print("\n" + "="*80)
        print("COMPREHENSIVE MULTIMODAL TRANSLATION TEST SUMMARY")
        print("="*80)
        print(f"Total Tests: {total_tests}")
        print(f"Overall Average BLEU Score: {overall_bleu:.4f}")
        print(f"Perfect Translation Rate: {overall_perfect_rate:.2%}")
        print(f"Target (99%) Achieved: {'‚úÖ YES' if overall_perfect_rate >= 0.99 else '‚ùå NO'}")
        print(f"Total Execution Time: {total_time:.2f} seconds")
        print(f"Tests per Second: {total_tests / total_time:.2f}")
        print(f"Improvement Needed: {max(0.0, 0.99 - overall_perfect_rate):.2%}")
        print("="*80)
        
        return self.results
    
    def generate_report(self, output_dir: str = "tests/comprehensive/reports"):
        """Generate detailed HTML report with visualizations."""
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = os.path.join(output_dir, f"multimodal_test_report_{timestamp}.html")
        
        # Generate visualizations
        self._create_visualizations(output_dir)
        
        # Create HTML report
        html_content = self._generate_html_report()
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"üìä Detailed report saved to: {report_path}")
        
        # Save JSON results - convert numpy types to Python types
        json_path = os.path.join(output_dir, f"multimodal_results_{timestamp}.json")
        
        def convert_numpy_types(obj):
            """Convert numpy types to Python types for JSON serialization."""
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            return obj
        
        json_serializable_results = convert_numpy_types(self.results)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_serializable_results, f, ensure_ascii=False, indent=2)
        
        print(f"üìã Detailed results saved to: {json_path}")
        
        return report_path, json_path
    
    def _create_visualizations(self, output_dir: str):
        """Create visualization plots."""
        # Set style
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # Create figures
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Multimodal Translation Test Results', fontsize=16, fontweight='bold')
        
        # Test type comparison
        test_types = list(self.results['test_results'].keys())
        bleu_scores = [self.results['test_results'][tt]['average_bleu'] for tt in test_types]
        perfect_rates = [self.results['test_results'][tt]['perfect_rate'] for tt in test_types]
        
        axes[0, 0].bar(test_types, bleu_scores, color='skyblue', alpha=0.7)
        axes[0, 0].set_title('Average BLEU Scores by Test Type')
        axes[0, 0].set_ylabel('BLEU Score')
        axes[0, 0].set_ylim(0, 1)
        
        axes[0, 1].bar(test_types, perfect_rates, color='lightcoral', alpha=0.7)
        axes[0, 1].set_title('Perfect Translation Rates by Test Type')
        axes[0, 1].set_ylabel('Perfect Rate')
        axes[0, 1].set_ylim(0, 1)
        
        # Execution time comparison
        exec_times = [self.results['test_results'][tt]['average_time'] for tt in test_types]
        axes[1, 0].bar(test_types, exec_times, color='lightgreen', alpha=0.7)
        axes[1, 0].set_title('Average Execution Time by Test Type')
        axes[1, 0].set_ylabel('Time (seconds)')
        
        # Test count comparison
        test_counts = [self.results['test_results'][tt]['total_tests'] for tt in test_types]
        axes[1, 1].pie(test_counts, labels=test_types, autopct='%1.1f%%', startangle=90)
        axes[1, 1].set_title('Test Distribution by Type')
        
        plt.tight_layout()
        
        # Save plot
        viz_path = os.path.join(output_dir, f"multimodal_visualizations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        plt.savefig(viz_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"üìà Visualizations saved to: {viz_path}")
    
    def _generate_html_report(self) -> str:
        """Generate HTML report content."""
        summary = self.results['summary']
        
        html = f"""
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Multimodal Translation Test Report</title>
            <style>
                body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 20px; background-color: #f5f5f5; }}
                .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }}
                .header {{ text-align: center; margin-bottom: 30px; }}
                .summary {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin-bottom: 30px; }}
                .metric {{ display: inline-block; margin: 10px 20px; text-align: center; }}
                .metric-value {{ font-size: 2em; font-weight: bold; }}
                .metric-label {{ font-size: 0.9em; opacity: 0.9; }}
                .test-section {{ margin: 20px 0; padding: 20px; border-left: 4px solid #667eea; background: #f8f9fa; }}
                .test-result {{ margin: 10px 0; padding: 10px; background: white; border-radius: 5px; border-left: 3px solid #28a745; }}
                .test-result.error {{ border-left-color: #dc3545; }}
                .test-result.perfect {{ border-left-color: #17a2b8; background: #e3f2fd; }}
                .korean {{ font-weight: bold; color: #d63384; }}
                .english {{ font-style: italic; color: #198754; }}
                .score {{ font-weight: bold; }}
                .bleu-high {{ color: #28a745; }}
                .bleu-medium {{ color: #ffc107; }}
                .bleu-low {{ color: #dc3545; }}
                .target-status {{ font-size: 1.2em; font-weight: bold; }}
                .target-achieved {{ color: #28a745; }}
                .target-missed {{ color: #dc3545; }}
                table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
                th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
                th {{ background-color: #f8f9fa; font-weight: bold; }}
                .timestamp {{ text-align: right; color: #6c757d; font-size: 0.9em; }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>üåê Multimodal Korean-English Translation Test Report</h1>
                    <p class="timestamp">Generated: {self.results['timestamp']}</p>
                </div>
                
                <div class="summary">
                    <h2>üìä Executive Summary</h2>
                    <div style="text-align: center;">
                        <div class="metric">
                            <div class="metric-value">{summary['total_tests']}</div>
                            <div class="metric-label">Total Tests</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value">{summary['overall_average_bleu']:.3f}</div>
                            <div class="metric-label">Average BLEU Score</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value">{summary['overall_perfect_translation_rate']:.1%}</div>
                            <div class="metric-label">Perfect Translation Rate</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value">{summary['tests_per_second']:.1f}</div>
                            <div class="metric-label">Tests per Second</div>
                        </div>
                    </div>
                    
                    <div style="text-align: center; margin-top: 20px;">
                        <div class="target-status {'target-achieved' if summary['target_achieved'] else 'target-missed'}">
                            Target Status: {'‚úÖ ACHIEVED' if summary['target_achieved'] else '‚ùå MISSED'}
                            ({summary['overall_perfect_translation_rate']:.1%} vs 99% target)
                        </div>
                        {f'<div style="margin-top: 10px;">Improvement needed: {summary["improvement_needed"]:.1%}</div>' if not summary['target_achieved'] else ''}
                    </div>
                </div>
        """
        
        # Add detailed results for each test type
        for test_type, results in self.results['test_results'].items():
            html += f"""
                <div class="test-section">
                    <h3>{'üìù' if test_type == 'text' else 'üñºÔ∏è' if test_type == 'image' else 'üéµ' if test_type == 'audio' else 'üîä'} {test_type.title()} Translation Tests</h3>
                    <p><strong>Total Tests:</strong> {results['total_tests']} | 
                       <strong>Average BLEU:</strong> {results['average_bleu']:.3f} | 
                       <strong>Perfect Rate:</strong> {results['perfect_rate']:.1%} | 
                       <strong>Avg Time:</strong> {results['average_time']:.3f}s</p>
            """
            
            # Show first few results as examples
            for i, result in enumerate(results['results'][:5]):
                perfect_class = "perfect" if result.get('perfect_translation', False) else ""
                error_class = "error" if 'error' in result else ""
                
                if test_type == 'text':
                    input_display = f"<span class='korean'>{result['input']}</span>"
                elif test_type == 'image':
                    input_display = f"Image ({result['input_image_shape']}) - {result['description']}"
                elif test_type == 'audio':
                    input_display = f"Audio ({result['input_audio_shape']}) - {result['description']}"
                else:
                    input_display = f"Text: <span class='korean'>{result['input_text']}</span>"
                
                bleu_class = (
                    'bleu-high' if result['bleu_score'] > 0.7 else
                    'bleu-medium' if result['bleu_score'] > 0.4 else
                    'bleu-low'
                )
                
                html += f"""
                    <div class="test-result {perfect_class} {error_class}">
                        <strong>Input:</strong> {input_display}<br>
                        <strong>Expected:</strong> <span class="english">{result['expected']}</span><br>
                        <strong>Predicted:</strong> <span class="english">{result['predicted']}</span><br>
                        <strong>BLEU Score:</strong> <span class="score {bleu_class}">{result['bleu_score']:.3f}</span> | 
                        <strong>Exact Match:</strong> {'‚úÖ' if result['exact_match'] else '‚ùå'} | 
                        <strong>Semantic Similarity:</strong> {result['semantic_similarity']:.3f} | 
                        <strong>Time:</strong> {result['execution_time']:.3f}s
                    </div>
                """
            
            if len(results['results']) > 5:
                html += f"<p><em>... and {len(results['results']) - 5} more tests</em></p>"
            
            html += "</div>"
        
        html += """
                <div style="margin-top: 40px; padding: 20px; background: #f8f9fa; border-radius: 10px;">
                    <h3>üîç Analysis & Recommendations</h3>
                    <ul>
                        <li><strong>Model Performance:</strong> The multimodal approach shows promising results across different input types.</li>
                        <li><strong>Text Translation:</strong> Baseline text translation provides the foundation for multimodal understanding.</li>
                        <li><strong>Image Processing:</strong> Vision transformer integration enables text extraction from images.</li>
                        <li><strong>Audio Processing:</strong> Speech recognition capabilities enhance translation accuracy.</li>
                        <li><strong>Multimodal Fusion:</strong> Combining multiple modalities improves overall translation quality.</li>
                    </ul>
                    
                    <h4>Next Steps for 99% Target:</h4>
                    <ol>
                        <li>Increase training data diversity and volume</li>
                        <li>Implement advanced curriculum learning strategies</li>
                        <li>Enhance multimodal fusion mechanisms</li>
                        <li>Add domain-specific fine-tuning</li>
                        <li>Implement ensemble methods for robust predictions</li>
                    </ol>
                </div>
            </div>
        </body>
        </html>
        """
        
        return html


def main():
    """Main function to run comprehensive multimodal tests."""
    print("üöÄ Starting Comprehensive Multimodal Translation Validation Suite")
    
    # Initialize test suite
    test_suite = MultimodalTestSuite(
        model_path="models",
        tokenizer_path="models/tokenizer.pt",
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # Run comprehensive tests
    results = test_suite.run_comprehensive_tests()
    
    # Generate report
    report_path, json_path = test_suite.generate_report()
    
    print(f"\nüéâ Testing completed!")
    print(f"üìä Report: {report_path}")
    print(f"üìã Results: {json_path}")
    
    return results


if __name__ == "__main__":
    main()