import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple, Optional
import json
import time
from pathlib import Path
import logging
from dataclasses import dataclass
from tqdm import tqdm

# Import our models
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.nmt_transformer import NMTTransformer
from src.models.image_encoder import EnhancedMultimodalNMT, KoreanTextImageEncoder
from src.models.audio_encoder import MultimodalAudioNMT, KoreanSpeechEncoder
from src.data.prepare_corpus import ParallelCorpusProcessor
from src.training.train_nmt import NMTTrainer
from src.utils.metrics import BLEUScore, ExactMatchScore
from src.models.sp_tokenizer import SPTokenizer as SentencePieceTokenizer


@dataclass
class TestResult:
    """Result of a single test case."""
    input_text: str
    input_image: Optional[np.ndarray]
    input_audio: Optional[np.ndarray]
    expected_translation: str
    predicted_translation: str
    bleu_score: float
    exact_match: bool
    execution_time: float
    modality: str  # 'text', 'image', 'audio', 'multimodal'


@dataclass
class ValidationReport:
    """Complete validation report."""
    total_tests: int
    passed_tests: int
    failed_tests: int
    average_bleu: float
    perfect_translation_rate: float
    average_execution_time: float
    modality_breakdown: Dict[str, Dict[str, float]]
    detailed_results: List[TestResult]
    timestamp: str


class MultimodalValidator:
    """
    Comprehensive validator for multimodal Korean-English translation.
    Tests text, image, audio, and multimodal inputs.
    """
    
    def __init__(self,
                 text_model_path: str,
                 image_model_path: Optional[str] = None,
                 audio_model_path: Optional[str] = None,
                 tokenizer_path: str = "models/tokenizers/korean_english_spm",
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
        
        self.device = device
        self.logger = logging.getLogger(__name__)
        
        # Load tokenizer
        self.tokenizer = SentencePieceTokenizer(model_path=tokenizer_path)
        
        # Load models
        self.text_model = self._load_text_model(text_model_path)
        self.image_model = self._load_image_model(image_model_path) if image_model_path else None
        self.audio_model = self._load_audio_model(audio_model_path) if audio_model_path else None
        
        # Metrics
        self.bleu_metric = BLEUScore()
        self.exact_match_metric = ExactMatchScore()
        
    def _load_text_model(self, model_path: str) -> NMTTransformer:
        """Load the text NMT model."""
        checkpoint = torch.load(model_path, map_location=self.device)
        
        model = NMTTransformer(
            src_vocab_size=checkpoint['config']['src_vocab_size'],
            tgt_vocab_size=checkpoint['config']['tgt_vocab_size'],
            d_model=checkpoint['config']['d_model'],
            n_heads=checkpoint['config']['n_heads'],
            n_encoder_layers=checkpoint['config']['n_encoder_layers'],
            n_decoder_layers=checkpoint['config']['n_decoder_layers'],
            d_ff=checkpoint['config']['d_ff'],
            max_len=checkpoint['config']['max_len'],
            dropout=checkpoint['config']['dropout'],
            pad_id=checkpoint['config']['pad_id'],
            use_flash=checkpoint['config']['use_flash']
        ).to(self.device)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        return model
        
    def _load_image_model(self, model_path: str) -> EnhancedMultimodalNMT:
        """Load the image-based NMT model."""
        checkpoint = torch.load(model_path, map_location=self.device)
        
        model = EnhancedMultimodalNMT(
            src_vocab_size=checkpoint['config']['src_vocab_size'],
            tgt_vocab_size=checkpoint['config']['tgt_vocab_size'],
            d_model=checkpoint['config']['d_model'],
            n_heads=checkpoint['config']['n_heads'],
            n_encoder_layers=checkpoint['config']['n_encoder_layers'],
            n_decoder_layers=checkpoint['config']['n_decoder_layers'],
            d_ff=checkpoint['config']['d_ff'],
            max_len=checkpoint['config']['max_len'],
            dropout=checkpoint['config']['dropout'],
            pad_id=checkpoint['config']['pad_id'],
            use_flash=checkpoint['config']['use_flash']
        ).to(self.device)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        return model
        
    def _load_audio_model(self, model_path: str) -> MultimodalAudioNMT:
        """Load the audio-based NMT model."""
        checkpoint = torch.load(model_path, map_location=self.device)
        
        model = MultimodalAudioNMT(
            src_vocab_size=checkpoint['config']['src_vocab_size'],
            tgt_vocab_size=checkpoint['config']['tgt_vocab_size'],
            d_model=checkpoint['config']['d_model'],
            n_heads=checkpoint['config']['n_heads'],
            n_encoder_layers=checkpoint['config']['n_encoder_layers'],
            n_decoder_layers=checkpoint['config']['n_decoder_layers'],
            d_ff=checkpoint['config']['d_ff'],
            max_len=checkpoint['config']['max_len'],
            dropout=checkpoint['config']['dropout'],
            pad_id=checkpoint['config']['pad_id'],
            use_flash=checkpoint['config']['use_flash']
        ).to(self.device)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        return model
        
    def create_test_image(self, text: str, image_size: int = 224) -> np.ndarray:
        """Create synthetic Korean text image for testing."""
        # This would use PIL/OpenCV to create actual images
        # For now, create random tensor as placeholder
        return np.random.randn(3, image_size, image_size).astype(np.float32)
        
    def create_test_audio(self, text: str, duration: float = 3.0, sample_rate: int = 16000) -> np.ndarray:
        """Create synthetic Korean speech audio for testing."""
        # This would use TTS to create actual audio
        # For now, create random tensor as placeholder
        audio_length = int(duration * sample_rate)
        return np.random.randn(audio_length).astype(np.float32)
        
    def test_text_translation(self, test_cases: List[Dict]) -> List[TestResult]:
        """Test text-only translation."""
        results = []
        
        for case in tqdm(test_cases, desc="Text Translation Tests"):
            start_time = time.time()
            
            # Tokenize input
            src_tokens = self.tokenizer.encode(case['korean'])
            src_tensor = torch.tensor([src_tokens]).to(self.device)
            
            # Generate translation
            with torch.no_grad():
                pred_tokens = self.text_model.generate(
                    src_tensor,
                    max_length=512,
                    beam_size=5,
                    temperature=1.0
                )
                
            # Decode prediction
            predicted = self.tokenizer.decode(pred_tokens[0].cpu().tolist())
            
            # Calculate metrics
            bleu_score = self.bleu_metric([predicted], [case['english']])
            exact_match = self.exact_match_metric(predicted, case['english'])
            
            execution_time = time.time() - start_time
            
            result = TestResult(
                input_text=case['korean'],
                input_image=None,
                input_audio=None,
                expected_translation=case['english'],
                predicted_translation=predicted,
                bleu_score=bleu_score,
                exact_match=exact_match,
                execution_time=execution_time,
                modality='text'
            )
            
            results.append(result)
            
        return results
        
    def test_image_translation(self, test_cases: List[Dict]) -> List[TestResult]:
        """Test image-based translation (Korean text in images)."""
        if not self.image_model:
            self.logger.warning("Image model not available, skipping image tests")
            return []
            
        results = []
        
        for case in tqdm(test_cases, desc="Image Translation Tests"):
            start_time = time.time()
            
            # Create test image
            test_image = self.create_test_image(case['korean'])
            image_tensor = torch.tensor(test_image).unsqueeze(0).to(self.device)
            
            # Create dummy text input (for batch compatibility)
            dummy_tokens = [self.tokenizer.pad_id] * 10
            src_tensor = torch.tensor([dummy_tokens]).to(self.device)
            
            # Generate translation
            self.image_model.set_mode('image')
            with torch.no_grad():
                pred_tokens = self.image_model.generate(
                    src_tensor,
                    src_images=image_tensor,
                    max_length=512,
                    beam_size=5,
                    temperature=1.0
                )
                
            # Decode prediction
            predicted = self.tokenizer.decode(pred_tokens[0].cpu().tolist())
            
            # Calculate metrics
            bleu_score = self.bleu_metric([predicted], [case['english']])
            exact_match = self.exact_match_metric(predicted, case['english'])
            
            execution_time = time.time() - start_time
            
            result = TestResult(
                input_text=case['korean'],
                input_image=test_image,
                input_audio=None,
                expected_translation=case['english'],
                predicted_translation=predicted,
                bleu_score=bleu_score,
                exact_match=exact_match,
                execution_time=execution_time,
                modality='image'
            )
            
            results.append(result)
            
        return results
        
    def test_audio_translation(self, test_cases: List[Dict]) -> List[TestResult]:
        """Test audio-based translation (Korean speech)."""
        if not self.audio_model:
            self.logger.warning("Audio model not available, skipping audio tests")
            return []
            
        results = []
        
        for case in tqdm(test_cases, desc="Audio Translation Tests"):
            start_time = time.time()
            
            # Create test audio
            test_audio = self.create_test_audio(case['korean'])
            audio_tensor = torch.tensor(test_audio).unsqueeze(0).to(self.device)
            
            # Create dummy text input (for batch compatibility)
            dummy_tokens = [self.tokenizer.pad_id] * 10
            src_tensor = torch.tensor([dummy_tokens]).to(self.device)
            
            # Generate translation
            self.audio_model.set_mode('audio')
            with torch.no_grad():
                pred_tokens = self.audio_model.generate(
                    src_tensor,
                    src_audio=audio_tensor,
                    max_length=512,
                    beam_size=5,
                    temperature=1.0
                )
                
            # Decode prediction
            predicted = self.tokenizer.decode(pred_tokens[0].cpu().tolist())
            
            # Calculate metrics
            bleu_score = self.bleu_metric([predicted], [case['english']])
            exact_match = self.exact_match_metric(predicted, case['english'])
            
            execution_time = time.time() - start_time
            
            result = TestResult(
                input_text=case['korean'],
                input_image=None,
                input_audio=test_audio,
                expected_translation=case['english'],
                predicted_translation=predicted,
                bleu_score=bleu_score,
                exact_match=exact_match,
                execution_time=execution_time,
                modality='audio'
            )
            
            results.append(result)
            
        return results
        
    def test_multimodal_translation(self, test_cases: List[Dict]) -> List[TestResult]:
        """Test multimodal translation (text + image/audio)."""
        results = []
        
        # Test text + image
        if self.image_model:
            for case in tqdm(test_cases[:len(test_cases)//2], desc="Multimodal Text+Image Tests"):
                start_time = time.time()
                
                # Create test image and tokenize text
                test_image = self.create_test_image(case['korean'])
                image_tensor = torch.tensor(test_image).unsqueeze(0).to(self.device)
                
                src_tokens = self.tokenizer.encode(case['korean'])
                src_tensor = torch.tensor([src_tokens]).to(self.device)
                
                # Generate translation
                self.image_model.set_mode('multimodal')
                with torch.no_grad():
                    pred_tokens = self.image_model.generate(
                        src_tensor,
                        src_images=image_tensor,
                        max_length=512,
                        beam_size=5,
                        temperature=1.0
                    )
                    
                # Decode prediction
                predicted = self.tokenizer.decode(pred_tokens[0].cpu().tolist())
                
                # Calculate metrics
                bleu_score = self.bleu_metric([predicted], [case['english']])
                exact_match = self.exact_match_metric(predicted, case['english'])
                
                execution_time = time.time() - start_time
                
                result = TestResult(
                    input_text=case['korean'],
                    input_image=test_image,
                    input_audio=None,
                    expected_translation=case['english'],
                    predicted_translation=predicted,
                    bleu_score=bleu_score,
                    exact_match=exact_match,
                    execution_time=execution_time,
                    modality='multimodal_text_image'
                )
                
                results.append(result)
                
        # Test text + audio
        if self.audio_model:
            for case in tqdm(test_cases[len(test_cases)//2:], desc="Multimodal Text+Audio Tests"):
                start_time = time.time()
                
                # Create test audio and tokenize text
                test_audio = self.create_test_audio(case['korean'])
                audio_tensor = torch.tensor(test_audio).unsqueeze(0).to(self.device)
                
                src_tokens = self.tokenizer.encode(case['korean'])
                src_tensor = torch.tensor([src_tokens]).to(self.device)
                
                # Generate translation
                self.audio_model.set_mode('multimodal')
                with torch.no_grad():
                    pred_tokens = self.audio_model.generate(
                        src_tensor,
                        src_audio=audio_tensor,
                        max_length=512,
                        beam_size=5,
                        temperature=1.0
                    )
                    
                # Decode prediction
                predicted = self.tokenizer.decode(pred_tokens[0].cpu().tolist())
                
                # Calculate metrics
                bleu_score = self.bleu_metric([predicted], [case['english']])
                exact_match = self.exact_match_metric(predicted, case['english'])
                
                execution_time = time.time() - start_time
                
                result = TestResult(
                    input_text=case['korean'],
                    input_image=None,
                    input_audio=test_audio,
                    expected_translation=case['english'],
                    predicted_translation=predicted,
                    bleu_score=bleu_score,
                    exact_match=exact_match,
                    execution_time=execution_time,
                    modality='multimodal_text_audio'
                )
                
                results.append(result)
                
        return results
        
    def run_comprehensive_validation(self, test_cases: List[Dict]) -> ValidationReport:
        """Run comprehensive validation across all modalities."""
        self.logger.info("Starting comprehensive multimodal validation...")
        
        all_results = []
        
        # Run tests for each modality
        text_results = self.test_text_translation(test_cases)
        image_results = self.test_image_translation(test_cases)
        audio_results = self.test_audio_translation(test_cases)
        multimodal_results = self.test_multimodal_translation(test_cases)
        
        # Combine all results
        all_results.extend(text_results)
        all_results.extend(image_results)
        all_results.extend(audio_results)
        all_results.extend(multimodal_results)
        
        # Calculate statistics
        total_tests = len(all_results)
        passed_tests = sum(1 for r in all_results if r.exact_match)
        failed_tests = total_tests - passed_tests
        average_bleu = np.mean([r.bleu_score for r in all_results])
        perfect_translation_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        average_execution_time = np.mean([r.execution_time for r in all_results])
        
        # Modality breakdown
        modality_breakdown = {}
        for modality in set(r.modality for r in all_results):
            modality_results = [r for r in all_results if r.modality == modality]
            modality_breakdown[modality] = {
                'total_tests': len(modality_results),
                'passed_tests': sum(1 for r in modality_results if r.exact_match),
                'average_bleu': np.mean([r.bleu_score for r in modality_results]),
                'perfect_rate': (sum(1 for r in modality_results if r.exact_match) / len(modality_results)) * 100 if modality_results else 0,
                'average_time': np.mean([r.execution_time for r in modality_results])
            }
        
        # Create report
        report = ValidationReport(
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            average_bleu=average_bleu,
            perfect_translation_rate=perfect_translation_rate,
            average_execution_time=average_execution_time,
            modality_breakdown=modality_breakdown,
            detailed_results=all_results,
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )
        
        return report
        
    def save_report(self, report: ValidationReport, output_path: str):
        """Save validation report to JSON and HTML."""
        # Save JSON report
        json_path = Path(output_path) / "multimodal_validation_report.json"
        
        # Convert to serializable format
        report_dict = {
            'total_tests': report.total_tests,
            'passed_tests': report.passed_tests,
            'failed_tests': report.failed_tests,
            'average_bleu': report.average_bleu,
            'perfect_translation_rate': report.perfect_translation_rate,
            'average_execution_time': report.average_execution_time,
            'modality_breakdown': report.modality_breakdown,
            'timestamp': report.timestamp,
            'detailed_results': [
                {
                    'input_text': r.input_text,
                    'expected_translation': r.expected_translation,
                    'predicted_translation': r.predicted_translation,
                    'bleu_score': r.bleu_score,
                    'exact_match': r.exact_match,
                    'execution_time': r.execution_time,
                    'modality': r.modality
                }
                for r in report.detailed_results
            ]
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report_dict, f, ensure_ascii=False, indent=2)
            
        # Save HTML report
        html_path = Path(output_path) / "multimodal_validation_report.html"
        html_content = self._generate_html_report(report)
        
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
            
        self.logger.info(f"Validation reports saved to {json_path} and {html_path}")
        
    def _generate_html_report(self, report: ValidationReport) -> str:
        """Generate HTML report."""
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Multimodal Korean-English Translation Validation Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f4f4f4; padding: 20px; border-radius: 5px; }}
                .summary {{ margin: 20px 0; }}
                .modality-section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                .test-case {{ margin: 10px 0; padding: 10px; background-color: #f9f9f9; border-radius: 3px; }}
                .pass {{ color: green; }}
                .fail {{ color: red; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>Multimodal Korean-English Translation Validation Report</h1>
                <p>Generated: {report.timestamp}</p>
            </div>
            
            <div class="summary">
                <h2>Overall Summary</h2>
                <table>
                    <tr><th>Metric</th><th>Value</th></tr>
                    <tr><td>Total Tests</td><td>{report.total_tests}</td></tr>
                    <tr><td>Passed Tests</td><td>{report.passed_tests}</td></tr>
                    <tr><td>Failed Tests</td><td>{report.failed_tests}</td></tr>
                    <tr><td>Average BLEU Score</td><td>{report.average_bleu:.4f}</td></tr>
                    <tr><td>Perfect Translation Rate</td><td>{report.perfect_translation_rate:.2f}%</td></tr>
                    <tr><td>Average Execution Time</td><td>{report.average_execution_time:.4f}s</td></tr>
                </table>
            </div>
            
            <div class="modality-section">
                <h2>Modality Breakdown</h2>
                {self._generate_modality_tables(report.modality_breakdown)}
            </div>
            
            <div class="modality-section">
                <h2>Detailed Test Results</h2>
                {self._generate_detailed_results(report.detailed_results)}
            </div>
        </body>
        </html>
        """
        
        return html
        
    def _generate_modality_tables(self, modality_breakdown: Dict) -> str:
        """Generate HTML tables for modality breakdown."""
        html = ""
        
        for modality, stats in modality_breakdown.items():
            html += f"""
            <h3>{modality.replace('_', ' ').title()}</h3>
            <table>
                <tr><th>Metric</th><th>Value</th></tr>
                <tr><td>Total Tests</td><td>{stats['total_tests']}</td></tr>
                <tr><td>Passed Tests</td><td>{stats['passed_tests']}</td></tr>
                <tr><td>Average BLEU Score</td><td>{stats['average_bleu']:.4f}</td></tr>
                <tr><td>Perfect Translation Rate</td><td>{stats['perfect_rate']:.2f}%</td></tr>
                <tr><td>Average Execution Time</td><td>{stats['average_time']:.4f}s</td></tr>
            </table>
            """
            
        return html
        
    def _generate_detailed_results(self, results: List[TestResult]) -> str:
        """Generate HTML for detailed results."""
        html = ""
        
        for i, result in enumerate(results[:50]):  # Show first 50 results
            status_class = "pass" if result.exact_match else "fail"
            status_text = "âœ“ PASS" if result.exact_match else "âœ— FAIL"
            
            html += f"""
            <div class="test-case">
                <h4>Test {i+1}: {result.modality.replace('_', ' ').title()} - <span class="{status_class}">{status_text}</span></h4>
                <p><strong>Korean Input:</strong> {result.input_text}</p>
                <p><strong>Expected English:</strong> {result.expected_translation}</p>
                <p><strong>Predicted English:</strong> {result.predicted_translation}</p>
                <p><strong>BLEU Score:</strong> {result.bleu_score:.4f} | 
                   <strong>Execution Time:</strong> {result.execution_time:.4f}s</p>
            </div>
            """
            
        return html


def create_comprehensive_test_suite() -> List[Dict]:
    """Create comprehensive Korean-English test cases."""
    test_cases = [
        # Basic greetings and common phrases
        {"korean": "ì•ˆë…•í•˜ì„¸ìš”", "english": "Hello"},
        {"korean": "ê°ì‚¬í•©ë‹ˆë‹¤", "english": "Thank you"},
        {"korean": "ì£„ì†¡í•©ë‹ˆë‹¤", "english": "Sorry"},
        {"korean": "ë„¤", "english": "Yes"},
        {"korean": "ì•„ë‹ˆìš”", "english": "No"},
        
        # Daily conversations
        {"korean": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”", "english": "The weather is nice today"},
        {"korean": "ë°¥ ë¨¹ì—ˆì–´ìš”?", "english": "Did you eat?"},
        {"korean": "ì–´ë”” ê°€ì„¸ìš”?", "english": "Where are you going?"},
        {"korean": "ìž˜ ì§€ë‚´ì…¨ì–´ìš”?", "english": "Have you been well?"},
        
        # Complex sentences
        {"korean": "ì €ëŠ” í•œêµ­ì–´ë¥¼ ë°°ìš°ê³  ìžˆì–´ìš”", "english": "I am learning Korean"},
        {"korean": "ì´ ì±…ì€ ì •ë§ í¥ë¯¸ë¡œì›Œìš”", "english": "This book is really interesting"},
        {"korean": "ë‚´ì¼ í•™êµì— ê°€ì•¼ í•´ìš”", "english": "I have to go to school tomorrow"},
        {"korean": "ì»¤í”¼ ë§ˆì‹œê³  ì‹¶ì–´ìš”", "english": "I want to drink coffee"},
        
        # Technical/business Korean
        {"korean": "íšŒì˜ëŠ” ì˜¤í›„ 3ì‹œì— ì‹œìž‘ë©ë‹ˆë‹¤", "english": "The meeting starts at 3 PM"},
        {"korean": "í”„ë¡œì íŠ¸ ì¼ì •ì„ í™•ì¸í•´ ì£¼ì„¸ìš”", "english": "Please check the project schedule"},
        {"korean": "ë³´ê³ ì„œë¥¼ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤", "english": "I need to submit the report"},
        
        # Cultural expressions
        {"korean": "ë§Žì´ ë“œì„¸ìš”", "english": "Please eat a lot"},
        {"korean": "ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤", "english": "Thank you for your hard work"},
        {"korean": "ë“¤ì–´ì˜¤ì„¸ìš”", "english": "Please come in"},
        
        # Question forms
        {"korean": "ì´ê²ƒì€ ë¬´ì—‡ìž…ë‹ˆê¹Œ?", "english": "What is this?"},
        {"korean": "ì–¸ì œ ë„ì°©í–ˆì–´ìš”?", "english": "When did you arrive?"},
        {"korean": "ì–´ë–»ê²Œ ê°€ìš”?", "english": "How do I get there?"},
        {"korean": "ëˆ„êµ¬ì„¸ìš”?", "english": "Who are you?"},
        
        # Past/present/future tenses
        {"korean": "ì–´ì œ ì˜í™”ë¥¼ ë´¤ì–´ìš”", "english": "I watched a movie yesterday"},
        {"korean": "ì§€ê¸ˆ ê³µë¶€í•˜ê³  ìžˆì–´ìš”", "english": "I am studying now"},
        {"korean": "ë‚´ì¼ ì¹œêµ¬ë¥¼ ë§Œë‚  ê±°ì˜ˆìš”", "english": "I will meet my friend tomorrow"},
        
        # Honorifics and politeness levels
        {"korean": "ì„ ìƒë‹˜, ì§ˆë¬¸ì´ ìžˆì–´ìš”", "english": "Teacher, I have a question"},
        {"korean": "ë¶€ëª¨ë‹˜ê»˜ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤", "english": "I thank my parents"},
        {"korean": "í• ì•„ë²„ì§€ê»˜ ì•ˆë¶€ë¥¼ ì „í•´ ì£¼ì„¸ìš”", "english": "Please give my regards to grandfather"},
        
        # Numbers and quantities
        {"korean": "ì‚¬ê³¼ ë‘ ê°œ ì£¼ì„¸ìš”", "english": "Please give me two apples"},
        {"korean": "ì‹œê°„ì´ ì–¼ë§ˆë‚˜ ê±¸ë ¤ìš”?", "english": "How long does it take?"},
        {"korean": "ê°€ê²©ì´ ì–¼ë§ˆì˜ˆìš”?", "english": "How much is the price?"},
        
        # Emotions and feelings
        {"korean": "ì •ë§ ê¸°ë»ìš”", "english": "I am really happy"},
        {"korean": "ì¡°ê¸ˆ ê±±ì •ë¼ìš”", "english": "I am a little worried"},
        {"korean": "ë„ˆë¬´ í”¼ê³¤í•´ìš”", "english": "I am very tired"},
        {"korean": "ì •ë§ ë†€ëžì–´ìš”", "english": "I was really surprised"},
        
        # Directions and locations
        {"korean": "í™”ìž¥ì‹¤ì€ ì–´ë”” ìžˆì–´ìš”?", "english": "Where is the bathroom?"},
        {"korean": "ì—¬ê¸°ì„œ ì–¼ë§ˆë‚˜ ë©€ì–´ìš”?", "english": "How far is it from here?"},
        {"korean": "ì™¼ìª½ìœ¼ë¡œ ê°€ì„¸ìš”", "english": "Go to the left"},
        {"korean": "ì§ì§„í•˜ì„¸ìš”", "english": "Go straight"},
        
        # Shopping and restaurants
        {"korean": "ì´ê±° ì£¼ë¬¸í• ê²Œìš”", "english": "I will order this"},
        {"korean": "ê³„ì‚°í•´ ì£¼ì„¸ìš”", "english": "Please calculate the bill"},
        {"korean": "ì˜ìˆ˜ì¦ ì£¼ì„¸ìš”", "english": "Please give me a receipt"},
        {"korean": "í¬ìž¥í•´ ì£¼ì„¸ìš”", "english": "Please wrap it up"},
        
        # Transportation
        {"korean": "ë²„ìŠ¤ë¥¼ íƒ€ì•¼ í•´ìš”", "english": "I need to take the bus"},
        {"korean": "ì§€í•˜ì² ì—­ì€ ì–´ë””ì— ìžˆì–´ìš”?", "english": "Where is the subway station?"},
        {"korean": "í‘œ í•œ ìž¥ ì£¼ì„¸ìš”", "english": "Please give me one ticket"},
        
        # Health and medical
        {"korean": "ì•„íŒŒìš”", "english": "I am sick"},
        {"korean": "ì•½ì´ í•„ìš”í•´ìš”", "english": "I need medicine"},
        {"korean": "ë³‘ì›ì— ê°€ì•¼ í•´ìš”", "english": "I need to go to the hospital"},
        
        # Weather and seasons
        {"korean": "ì˜¤ëŠ˜ ë¹„ê°€ ì˜¬ ê±°ì˜ˆìš”", "english": "It will rain today"},
        {"korean": "ë„ˆë¬´ ì¶”ì›Œìš”", "english": "It is very cold"},
        {"korean": "ë´„ì´ ì™”ì–´ìš”", "english": "Spring has come"},
        
        # Family and relationships
        {"korean": "ê°€ì¡±ì´ ëª‡ ëª…ì´ì—ìš”?", "english": "How many family members do you have?"},
        {"korean": "í˜•ì œê°€ ìžˆì–´ìš”?", "english": "Do you have siblings?"},
        {"korean": "ê²°í˜¼í–ˆì–´ìš”?", "english": "Are you married?"},
        
        # Work and study
        {"korean": "ë¬´ìŠ¨ ì¼ì„ í•˜ì„¸ìš”?", "english": "What do you do for work?"},
        {"korean": "ì–´ë””ì„œ ì¼í•˜ì„¸ìš”?", "english": "Where do you work?"},
        {"korean": "í•œêµ­ì–´ë¥¼ ì–¼ë§ˆë‚˜ ê³µë¶€í–ˆì–´ìš”?", "english": "How long have you studied Korean?"},
        
        # Hobbies and interests
        {"korean": "ì·¨ë¯¸ê°€ ë­ì˜ˆìš”?", "english": "What is your hobby?"},
        {"korean": "ìŒì•…ì„ ì¢‹ì•„í•´ìš”", "english": "I like music"},
        {"korean": "ìš´ë™ì„ ìžì£¼ í•´ìš”", "english": "I exercise often"},
        
        # Time and dates
        {"korean": "ì§€ê¸ˆ ëª‡ ì‹œì˜ˆìš”?", "english": "What time is it now?"},
        {"korean": "ì˜¤ëŠ˜ì€ ë©°ì¹ ì´ì—ìš”?", "english": "What is today's date?"},
        {"korean": "ìƒì¼ì´ ì–¸ì œì˜ˆìš”?", "english": "When is your birthday?"},
        
        # Descriptions and comparisons
        {"korean": "ì´ê²ƒë³´ë‹¤ ì €ê²ƒì´ ë” ì¢‹ì•„ìš”", "english": "That is better than this"},
        {"korean": "ê°€ìž¥ ì¢‹ì•„í•˜ëŠ” ê²ƒì€ ë­ì˜ˆìš”?", "english": "What do you like the most?"},
        {"korean": "ì´ê²Œ ë” ì‹¸ìš”", "english": "This is cheaper"}
    ]
    
    return test_cases


def main():
    """Main validation function."""
    # Setup logging
    logging.basicConfig(level=logging.INFO)
    
    # Create test cases
    print("Creating comprehensive test suite...")
    test_cases = create_comprehensive_test_suite()
    print(f"Created {len(test_cases)} test cases")
    
    # Initialize validator
    print("Initializing multimodal validator...")
    validator = MultimodalValidator(
        text_model_path="models/checkpoints/nmt_transformer_best.pt",
        image_model_path="models/checkpoints/multimodal_image_best.pt",
        audio_model_path="models/checkpoints/multimodal_audio_best.pt",
        tokenizer_path="models/tokenizers/korean_english_spm",
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # Run validation
    print("Running comprehensive validation...")
    report = validator.run_comprehensive_validation(test_cases)
    
    # Print summary
    print("\n" + "="*60)
    print("MULTIMODAL VALIDATION SUMMARY")
    print("="*60)
    print(f"Total Tests: {report.total_tests}")
    print(f"Passed Tests: {report.passed_tests}")
    print(f"Failed Tests: {report.failed_tests}")
    print(f"Average BLEU Score: {report.average_bleu:.4f}")
    print(f"Perfect Translation Rate: {report.perfect_translation_rate:.2f}%")
    print(f"Average Execution Time: {report.average_execution_time:.4f}s")
    print("\nModality Breakdown:")
    
    for modality, stats in report.modality_breakdown.items():
        print(f"  {modality}:")
        print(f"    Tests: {stats['total_tests']}, Passed: {stats['passed_tests']}")
        print(f"    Avg BLEU: {stats['average_bleu']:.4f}, Perfect Rate: {stats['perfect_rate']:.2f}%")
        
    print("\n" + "="*60)
    
    # Save report
    output_dir = Path("tests/multimodal_reports")
    output_dir.mkdir(parents=True, exist_ok=True)
    validator.save_report(report, str(output_dir))
    
    # Check if target is achieved
    if report.perfect_translation_rate >= 99.0:
        print("ðŸŽ‰ TARGET ACHIEVED! 99% perfect translation rate reached!")
    else:
        improvement_needed = 99.0 - report.perfect_translation_rate
        print(f"ðŸ“ˆ Need {improvement_needed:.2f}% improvement to reach 99% target")
        
    return report


if __name__ == "__main__":
    main()