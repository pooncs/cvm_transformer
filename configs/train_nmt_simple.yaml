# Optimized training configuration for 99% perfect translation score
d_model: 1024  # Increased from 768 for better representation
n_heads: 16
n_encoder_layers: 12
n_decoder_layers: 12
d_ff: 4096
dropout: 0.1
max_len: 512
vocab_size: 600  # Updated to match our tokenizer

# Training parameters
batch_size: 32
learning_rate: 5e-5
total_iterations: 2000  # Reduced for small dataset
validation_frequency: 100
warmup_steps: 200
gradient_clip_norm: 1.0
label_smoothing: 0.1

# Model architecture
use_flash_attention: true
use_mixed_precision: true
use_gradient_accumulation: true
gradient_accumulation_steps: 2

# Evaluation settings
eval_beam_size: 5
eval_length_penalty: 1.0
generate_max_length: 128