# Training configuration for NMT model
d_model: 512  # Reduced for small dataset
n_heads: 8
n_encoder_layers: 6
n_decoder_layers: 6
d_ff: 2048
dropout: 0.1
max_len: 128

# Training parameters
batch_size: 16
gradient_accumulation_steps: 2
learning_rate: 0.0005
weight_decay: 0.01
warmup_steps: 200
max_steps: 1000  # Reduced for quick training
label_smoothing: 0.1
gradient_clip_norm: 1.0

# Curriculum learning
curriculum_stages:
  - name: "easy"
    difficulty: easy
    max_length: 32
    weight: 0.3
  - name: "medium"
    difficulty: medium
    max_length: 64
    weight: 0.5
  - name: "hard"
    difficulty: hard
    max_length: 128
    weight: 0.2
use_curriculum: true

# Knowledge distillation (disabled for now)
use_distillation: false
teacher_model_path: ""
distillation_temperature: 4.0
distillation_alpha: 0.5

# Data augmentation
use_augmentation: true
back_translation_weight: 0.3
noise_prob: 0.1

# Optimization
use_mixed_precision: true
use_flash_attention: true

# Evaluation
eval_steps: 100
save_steps: 200
logging_steps: 50